{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "聚类测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.927527 116.338899\n"
     ]
    }
   ],
   "source": [
    "# 数据处理\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Enable inline plotting\n",
    "names = ['lat','lng','zero','alt','days','date','time']\n",
    "streams = []\n",
    "index = 0\n",
    "\n",
    "userdata = 'D:\\\\cxcyProject\\\\Geolife Trajectories 1.3\\\\Data\\\\' + '002' + '\\\\Trajectory\\\\'\n",
    "filelist = os.listdir(userdata)\n",
    "\n",
    "for f in filelist:\n",
    "    df_list = [pd.read_csv(userdata + f,header=6,names=names,index_col=False)]\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    df.drop(['zero','alt','days','date','time'], axis=1, inplace=True)\n",
    "    df_min = df.iloc[::12, :]\n",
    "    lat_lng_data = np.c_[df_min['lat'].values, df_min['lng'].values]\n",
    "    streams.append(lat_lng_data)\n",
    "\n",
    "print(streams[0][0,0], streams[0][0,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb. clusters: 34\n"
     ]
    }
   ],
   "source": [
    "# 分类\n",
    "import geopy.distance\n",
    "from dipy.segment.metric import Metric\n",
    "from dipy.segment.clustering import ResampleFeature\n",
    "import numpy as np\n",
    "from dipy.segment.clustering import QuickBundles\n",
    "THRESHOLD = 2\n",
    "class GPSDistance(Metric):\n",
    "    def __init__(self):\n",
    "        super(GPSDistance, self).__init__(feature=ResampleFeature(nb_points=256))\n",
    "\n",
    "    def are_compatible(self, shape1, shape2):\n",
    "        return len(shape1) == len(shape2)\n",
    "\n",
    "    def dist(self, v1, v2):\n",
    "        x = [geopy.distance.distance([p[0][0], p[0][1]], [p[1][0], p[1][1]]).kilometers for p in list(zip(v1, v2))]\n",
    "        currD = np.mean(x)\n",
    "        return currD\n",
    "\n",
    "\n",
    "metric = GPSDistance()\n",
    "qb = QuickBundles(threshold=THRESHOLD, metric=metric)\n",
    "\n",
    "clusters = qb.cluster(streams)\n",
    "print(\"Nb. clusters:\", len(clusters))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画图\n",
    "from gmplot import gmplot\n",
    "import random\n",
    "\n",
    "def randomcolor():\n",
    "    colorArr = ['1','2','3','4','5','6','7','8','9','A','B','C','D','E','F']\n",
    "    color = \"\"\n",
    "    for i in range(6):\n",
    "        color += colorArr[random.randint(0,14)]\n",
    "    return \"#\"+color\n",
    "\n",
    "gmap = gmplot.GoogleMapPlotter(streams[0][0,0], streams[0][0,1], 12)\n",
    "\n",
    "for clustersIndex in range(7):\n",
    "    color = randomcolor()\n",
    "    for i in clusters[clustersIndex].indices:\n",
    "        gmap.plot(streams[i][:,0], streams[i][:,1], color, edge_width=1)\n",
    "\n",
    "gmap.draw(\"user002_map.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一个轨迹点的坐标: 39.984198, 116.319322\n"
     ]
    }
   ],
   "source": [
    "# 数据处理\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Enable inline plotting\n",
    "names = ['lat','lng','zero','alt','days','date','time']\n",
    "streams = []\n",
    "\n",
    "base_path = 'D:\\\\cxcyProject\\\\Geolife Trajectories 1.3\\\\Data'\n",
    "\n",
    "# 遍历所有用户（001到180）\n",
    "for user_id in range(1, 100):\n",
    "    # 格式化为三位数用户ID\n",
    "    user_folder = f\"{user_id:03d}\"\n",
    "    user_path = os.path.join(base_path, user_folder, 'Trajectory')\n",
    "    \n",
    "    # 检查路径是否存在\n",
    "    if not os.path.exists(user_path):\n",
    "        print(f\"跳过不存在的路径: {user_path}\")\n",
    "        continue\n",
    "    \n",
    "    # 获取当前用户的所有轨迹文件\n",
    "    filelist = os.listdir(user_path)\n",
    "    \n",
    "    # 处理每个轨迹文件\n",
    "    for f in filelist:\n",
    "        full_path = os.path.join(user_path, f)\n",
    "        try:\n",
    "            # 读取数据并处理\n",
    "            df = pd.read_csv(full_path, header=6, names=names, index_col=False)\n",
    "            df.drop(['zero','alt','days','date','time'], axis=1, inplace=True)\n",
    "            # 每12行采样一次\n",
    "            df_min = df.iloc[::25, :]\n",
    "            lat_lng_data = np.c_[df_min['lat'].values, df_min['lng'].values]\n",
    "            streams.append(lat_lng_data)\n",
    "        except Exception as e:\n",
    "            print(f\"处理文件 {full_path} 时出错: {str(e)}\")\n",
    "\n",
    "# 打印测试数据（如果存在数据）\n",
    "if streams and len(streams[0]) > 0:\n",
    "    print(f\"第一个轨迹点的坐标: {streams[0][0,0]}, {streams[0][0,1]}\")\n",
    "else:\n",
    "    print(\"没有找到有效数据\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一个轨迹点的坐标: 39.9717, 116.313113333333\n",
      "共处理了 30 个用户的 3319 条轨迹\n"
     ]
    }
   ],
   "source": [
    "# 数据处理\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import random  # 新增随机模块\n",
    "\n",
    "# Enable inline plotting\n",
    "names = ['lat','lng','zero','alt','days','date','time']\n",
    "streams = []\n",
    "\n",
    "base_path = 'D:\\\\cxcyProject\\\\Geolife Trajectories 1.3\\\\Data'\n",
    "\n",
    "# 随机选择30个用户（从001到180）\n",
    "selected_users = random.sample(range(1, 181), 30)  # 生成30个不重复的随机数\n",
    "\n",
    "for user_id in selected_users:\n",
    "    # 格式化为三位数用户ID\n",
    "    user_folder = f\"{user_id:03d}\"\n",
    "    user_path = os.path.join(base_path, user_folder, 'Trajectory')\n",
    "    \n",
    "    # 检查路径是否存在\n",
    "    if not os.path.exists(user_path):\n",
    "        print(f\"跳过不存在的路径: {user_path}\")\n",
    "        continue\n",
    "    \n",
    "    # 获取当前用户的所有轨迹文件\n",
    "    try:\n",
    "        filelist = os.listdir(user_path)\n",
    "    except Exception as e:\n",
    "        print(f\"无法读取目录 {user_path}: {str(e)}\")\n",
    "        continue\n",
    "    \n",
    "    # 处理每个轨迹文件\n",
    "    for filename in filelist:\n",
    "        file_path = os.path.join(user_path, filename)\n",
    "        try:\n",
    "            # 读取数据并处理\n",
    "            df = pd.read_csv(file_path, header=6, names=names, index_col=False)\n",
    "            df.drop(['zero','alt','days','date','time'], axis=1, inplace=True)\n",
    "            # 每20行采样一次\n",
    "            df_min = df.iloc[::40, :]\n",
    "            lat_lng_data = np.c_[df_min['lat'].values, df_min['lng'].values]\n",
    "            streams.append(lat_lng_data)\n",
    "        except Exception as e:\n",
    "            print(f\"处理文件 {file_path} 时出错: {str(e)}\")\n",
    "\n",
    "# 打印测试数据（如果存在数据）\n",
    "if streams:\n",
    "    print(f\"第一个轨迹点的坐标: {streams[0][0,0]}, {streams[0][0,1]}\")\n",
    "    print(f\"共处理了 {len(selected_users)} 个用户的 {len(streams)} 条轨迹\")\n",
    "else:\n",
    "    print(\"没有找到有效数据\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb. clusters: 341\n"
     ]
    }
   ],
   "source": [
    "# 分类\n",
    "import geopy.distance\n",
    "from dipy.segment.metric import Metric\n",
    "from dipy.segment.clustering import ResampleFeature\n",
    "import numpy as np\n",
    "from dipy.segment.clustering import QuickBundles\n",
    "THRESHOLD = 2.5\n",
    "class GPSDistance(Metric):\n",
    "    def __init__(self):\n",
    "        super(GPSDistance, self).__init__(feature=ResampleFeature(nb_points=256))\n",
    "\n",
    "    def are_compatible(self, shape1, shape2):\n",
    "        return len(shape1) == len(shape2)\n",
    "\n",
    "    def dist(self, v1, v2):\n",
    "        x = [geopy.distance.distance([p[0][0], p[0][1]], [p[1][0], p[1][1]]).kilometers for p in list(zip(v1, v2))]\n",
    "        currD = np.mean(x)\n",
    "        return currD\n",
    "\n",
    "\n",
    "metric = GPSDistance()\n",
    "qb = QuickBundles(threshold=THRESHOLD, metric=metric)\n",
    "\n",
    "clusters = qb.cluster(streams)\n",
    "print(\"Nb. clusters:\", len(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画图\n",
    "from gmplot import gmplot\n",
    "import random\n",
    "\n",
    "def randomcolor():\n",
    "    colorArr = ['1','2','3','4','5','6','7','8','9','A','B','C','D','E','F']\n",
    "    color = \"\"\n",
    "    for i in range(6):\n",
    "        color += colorArr[random.randint(0,14)]\n",
    "    return \"#\"+color\n",
    "\n",
    "gmap = gmplot.GoogleMapPlotter(streams[0][0,0], streams[0][0,1], 12)\n",
    "\n",
    "for clustersIndex in range(7):\n",
    "    color = randomcolor()\n",
    "    for i in clusters[clustersIndex].indices:\n",
    "        gmap.plot(streams[i][:,0], streams[i][:,1], color, edge_width=1)\n",
    "\n",
    "gmap.draw(\"user004_map.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZYPC\\AppData\\Local\\Temp\\ipykernel_24996\\1568629575.py:38: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  norm_traj[:,0] = (traj[:,0] - self.min_lat) / (self.max_lat - self.min_lat)\n",
      "C:\\Users\\ZYPC\\AppData\\Local\\Temp\\ipykernel_24996\\1568629575.py:38: RuntimeWarning: overflow encountered in subtract\n",
      "  norm_traj[:,0] = (traj[:,0] - self.min_lat) / (self.max_lat - self.min_lat)\n",
      "C:\\Users\\ZYPC\\AppData\\Local\\Temp\\ipykernel_24996\\1568629575.py:38: RuntimeWarning: invalid value encountered in divide\n",
      "  norm_traj[:,0] = (traj[:,0] - self.min_lat) / (self.max_lat - self.min_lat)\n",
      "C:\\Users\\ZYPC\\AppData\\Local\\Temp\\ipykernel_24996\\1568629575.py:38: RuntimeWarning: invalid value encountered in subtract\n",
      "  norm_traj[:,0] = (traj[:,0] - self.min_lat) / (self.max_lat - self.min_lat)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: nan\n",
      "Epoch 2/50, Loss: nan\n",
      "Epoch 3/50, Loss: nan\n",
      "Epoch 4/50, Loss: nan\n",
      "Epoch 5/50, Loss: nan\n",
      "Epoch 6/50, Loss: nan\n",
      "Epoch 7/50, Loss: nan\n",
      "Epoch 8/50, Loss: nan\n",
      "Epoch 9/50, Loss: nan\n",
      "Epoch 10/50, Loss: nan\n",
      "Epoch 11/50, Loss: nan\n",
      "Epoch 12/50, Loss: nan\n",
      "Epoch 13/50, Loss: nan\n",
      "Epoch 14/50, Loss: nan\n",
      "Epoch 15/50, Loss: nan\n",
      "Epoch 16/50, Loss: nan\n",
      "Epoch 17/50, Loss: nan\n",
      "Epoch 18/50, Loss: nan\n",
      "Epoch 19/50, Loss: nan\n",
      "Epoch 20/50, Loss: nan\n",
      "Epoch 21/50, Loss: nan\n",
      "Epoch 22/50, Loss: nan\n",
      "Epoch 23/50, Loss: nan\n",
      "Epoch 24/50, Loss: nan\n",
      "Epoch 25/50, Loss: nan\n",
      "Epoch 26/50, Loss: nan\n",
      "Epoch 27/50, Loss: nan\n",
      "Epoch 28/50, Loss: nan\n",
      "Epoch 29/50, Loss: nan\n",
      "Epoch 30/50, Loss: nan\n",
      "Epoch 31/50, Loss: nan\n",
      "Epoch 32/50, Loss: nan\n",
      "Epoch 33/50, Loss: nan\n",
      "Epoch 34/50, Loss: nan\n",
      "Epoch 35/50, Loss: nan\n",
      "Epoch 36/50, Loss: nan\n",
      "Epoch 37/50, Loss: nan\n",
      "Epoch 38/50, Loss: nan\n",
      "Epoch 39/50, Loss: nan\n",
      "Epoch 40/50, Loss: nan\n",
      "Epoch 41/50, Loss: nan\n",
      "Epoch 42/50, Loss: nan\n",
      "Epoch 43/50, Loss: nan\n",
      "Epoch 44/50, Loss: nan\n",
      "Epoch 45/50, Loss: nan\n",
      "Epoch 46/50, Loss: nan\n",
      "Epoch 47/50, Loss: nan\n",
      "Epoch 48/50, Loss: nan\n",
      "Epoch 49/50, Loss: nan\n",
      "Epoch 50/50, Loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mdipy/segment/metricspeed.pyx:59\u001b[0m, in \u001b[0;36mdipy.segment.metricspeed.Metric.c_dist\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mdipy/segment/metricspeed.pyx:81\u001b[0m, in \u001b[0;36mdipy.segment.metricspeed.Metric.dist\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[3], line 98\u001b[0m, in \u001b[0;36mDeepTrajectoryMetric.dist\u001b[1;34m(self, v1, v2)\u001b[0m\n\u001b[0;32m     95\u001b[0m tensor_v2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(v2\u001b[38;5;241m.\u001b[39mflatten())\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# 提取潜在特征\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m z1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(tensor_v1)\n\u001b[0;32m     99\u001b[0m z2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(tensor_v2)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# 计算余弦相似度\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programs Files\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Programs Files\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Programs Files\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32md:\\Programs Files\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Programs Files\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Programs Files\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'dipy.segment.clusteringspeed.QuickBundles.find_nearest_cluster'\n",
      "Traceback (most recent call last):\n",
      "  File \"dipy/segment/metricspeed.pyx\", line 59, in dipy.segment.metricspeed.Metric.c_dist\n",
      "  File \"dipy/segment/metricspeed.pyx\", line 81, in dipy.segment.metricspeed.Metric.dist\n",
      "  File \"C:\\Users\\ZYPC\\AppData\\Local\\Temp\\ipykernel_24996\\1568629575.py\", line 98, in dist\n",
      "  File \"d:\\Programs Files\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Programs Files\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Programs Files\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py\", line 250, in forward\n",
      "    input = module(input)\n",
      "            ^^^^^^^^^^^^^\n",
      "  File \"d:\\Programs Files\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Programs Files\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Programs Files\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 125, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "# 深度学习模块\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# 原有模块\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import geopy.distance\n",
    "from dipy.segment.metric import Metric\n",
    "from dipy.segment.clustering import ResampleFeature, QuickBundles\n",
    "\n",
    "# 超参数配置\n",
    "THRESHOLD = 2.5\n",
    "LATENT_DIM = 64  # 潜在空间维度\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 改进1：轨迹数据集类（含数据标准化）\n",
    "# --------------------------------------------------\n",
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, streams):\n",
    "        # 数据标准化：将经纬度归一化到[0,1]范围\n",
    "        self.min_lat = min(traj[:,0].min() for traj in streams)\n",
    "        self.max_lat = max(traj[:,0].max() for traj in streams)\n",
    "        self.min_lng = min(traj[:,1].min() for traj in streams)\n",
    "        self.max_lng = max(traj[:,1].max() for traj in streams)\n",
    "        \n",
    "        self.normalized = []\n",
    "        for traj in streams:\n",
    "            norm_traj = np.empty_like(traj)\n",
    "            norm_traj[:,0] = (traj[:,0] - self.min_lat) / (self.max_lat - self.min_lat)\n",
    "            norm_traj[:,1] = (traj[:,1] - self.min_lng) / (self.max_lng - self.min_lng)\n",
    "            self.normalized.append(norm_traj)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.normalized)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.FloatTensor(self.normalized[idx].flatten()).to(DEVICE)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 改进2：时空自编码器模型\n",
    "# --------------------------------------------------\n",
    "class SpatioTemporalAE(nn.Module):\n",
    "    def __init__(self, input_dim=256*2):\n",
    "        super().__init__()\n",
    "        # 编码器：捕捉时空模式\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, LATENT_DIM))\n",
    "        \n",
    "        # 解码器：重建轨迹\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(LATENT_DIM, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, input_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        recon = self.decoder(z)\n",
    "        return z, recon\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 改进3：深度学习增强的度量标准\n",
    "# --------------------------------------------------\n",
    "class DeepTrajectoryMetric(Metric):\n",
    "    def __init__(self, encoder_model):\n",
    "        super().__init__(feature=ResampleFeature(nb_points=256))\n",
    "        self.encoder = encoder_model\n",
    "        self.encoder.eval()  # 冻结模型\n",
    "        \n",
    "    def are_compatible(self, shape1, shape2):\n",
    "        return len(shape1) == len(shape2)\n",
    "    \n",
    "    def dist(self, v1, v2):\n",
    "        with torch.no_grad():\n",
    "            # 将numpy数组转换为张量\n",
    "            tensor_v1 = torch.FloatTensor(v1.flatten()).unsqueeze(0).to(DEVICE)\n",
    "            tensor_v2 = torch.FloatTensor(v2.flatten()).unsqueeze(0).to(DEVICE)\n",
    "            \n",
    "            # 提取潜在特征\n",
    "            z1 = self.encoder(tensor_v1)\n",
    "            z2 = self.encoder(tensor_v2)\n",
    "            \n",
    "            # 计算余弦相似度\n",
    "            return 1 - torch.cosine_similarity(z1, z2).item()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 主流程修改\n",
    "# --------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # ========== 数据加载 ==========\n",
    "    # （原始数据加载部分保持不变）\n",
    "    # ... [原有数据加载代码] ...\n",
    "    \n",
    "    # ========== 数据预处理 ==========\n",
    "    # 确保所有轨迹统一长度\n",
    "    resampler = ResampleFeature(nb_points=256)\n",
    "    streams = [resampler.extract(traj) for traj in streams]\n",
    "    \n",
    "    # 创建数据集\n",
    "    dataset = TrajectoryDataset(streams)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    # ========== 模型训练 ==========\n",
    "    model = SpatioTemporalAE().to(DEVICE)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    \n",
    "    # 训练循环\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            _, recon = model(batch)\n",
    "            loss = criterion(recon, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "    \n",
    "    # ========== 深度聚类 ==========\n",
    "    # 创建深度学习度量标准\n",
    "    metric = DeepTrajectoryMetric(model.encoder)\n",
    "    \n",
    "    # 初始化QuickBundles\n",
    "    qb = QuickBundles(threshold=THRESHOLD, metric=metric)\n",
    "    \n",
    "    # 执行聚类（使用原始streams，因Resample已在预处理完成）\n",
    "    clusters = qb.cluster(streams)\n",
    "    print(\"Nb. clusters:\", len(clusters))\n",
    "    \n",
    "    # ========== 可视化（可选） =========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一个轨迹点的坐标: 26.161528, 119.943234\n",
      "共处理了 3 个用户的 72 条轨迹\n"
     ]
    }
   ],
   "source": [
    "# 数据处理\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import random  # 新增随机模块\n",
    "\n",
    "# Enable inline plotting\n",
    "names = ['lat','lng','zero','alt','days','date','time']\n",
    "streams = []\n",
    "\n",
    "base_path = 'D:\\\\cxcyProject\\\\Geolife Trajectories 1.3\\\\Data'\n",
    "\n",
    "# 随机选择30个用户（从001到180）\n",
    "selected_users = random.sample(range(1, 181), 3)  # 生成30个不重复的随机数\n",
    "\n",
    "for user_id in selected_users:\n",
    "    # 格式化为三位数用户ID\n",
    "    user_folder = f\"{user_id:03d}\"\n",
    "    user_path = os.path.join(base_path, user_folder, 'Trajectory')\n",
    "    \n",
    "    # 检查路径是否存在\n",
    "    if not os.path.exists(user_path):\n",
    "        print(f\"跳过不存在的路径: {user_path}\")\n",
    "        continue\n",
    "    \n",
    "    # 获取当前用户的所有轨迹文件\n",
    "    try:\n",
    "        filelist = os.listdir(user_path)\n",
    "    except Exception as e:\n",
    "        print(f\"无法读取目录 {user_path}: {str(e)}\")\n",
    "        continue\n",
    "    \n",
    "    # 处理每个轨迹文件\n",
    "    for filename in filelist:\n",
    "        file_path = os.path.join(user_path, filename)\n",
    "        try:\n",
    "            # 读取数据并处理\n",
    "            df = pd.read_csv(file_path, header=6, names=names, index_col=False)\n",
    "            df.drop(['zero','alt','days','date','time'], axis=1, inplace=True)\n",
    "            # 每20行采样一次\n",
    "            df_min = df.iloc[::20, :]\n",
    "            lat_lng_data = np.c_[df_min['lat'].values, df_min['lng'].values]\n",
    "            streams.append(lat_lng_data)\n",
    "        except Exception as e:\n",
    "            print(f\"处理文件 {file_path} 时出错: {str(e)}\")\n",
    "\n",
    "# 打印测试数据（如果存在数据）\n",
    "if streams:\n",
    "    print(f\"第一个轨迹点的坐标: {streams[0][0,0]}, {streams[0][0,1]}\")\n",
    "    print(f\"共处理了 {len(selected_users)} 个用户的 {len(streams)} 条轨迹\")\n",
    "else:\n",
    "    print(\"没有找到有效数据\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4855\n",
      "Epoch 2, Loss: 0.4218\n",
      "Epoch 3, Loss: 0.2345\n",
      "Epoch 4, Loss: 0.1005\n",
      "Epoch 5, Loss: 0.0311\n",
      "Epoch 6, Loss: 0.0317\n",
      "Epoch 7, Loss: 0.0259\n",
      "Epoch 8, Loss: 0.0216\n",
      "Epoch 9, Loss: 0.0177\n",
      "Epoch 10, Loss: 0.0100\n",
      "Epoch 11, Loss: 0.0090\n",
      "Epoch 12, Loss: 0.0067\n",
      "Epoch 13, Loss: 0.0062\n",
      "Epoch 14, Loss: 0.0040\n",
      "Epoch 15, Loss: 0.0044\n",
      "Epoch 16, Loss: 0.0031\n",
      "Epoch 17, Loss: 0.0033\n",
      "Epoch 18, Loss: 0.0027\n",
      "Epoch 19, Loss: 0.0028\n",
      "Epoch 20, Loss: 0.0025\n",
      "Epoch 21, Loss: 0.0076\n",
      "Epoch 22, Loss: 0.0024\n",
      "Epoch 23, Loss: 0.0086\n",
      "Epoch 24, Loss: 0.0037\n",
      "Epoch 25, Loss: 0.0022\n",
      "Epoch 26, Loss: 0.0022\n",
      "Epoch 27, Loss: 0.0022\n",
      "Epoch 28, Loss: 0.0022\n",
      "Epoch 29, Loss: 0.0022\n",
      "Epoch 30, Loss: 0.0072\n",
      "Epoch 31, Loss: 0.0023\n",
      "Epoch 32, Loss: 0.0023\n",
      "Epoch 33, Loss: 0.0035\n",
      "Epoch 34, Loss: 0.0022\n",
      "Epoch 35, Loss: 0.0022\n",
      "Epoch 36, Loss: 0.0022\n",
      "Epoch 37, Loss: 0.0021\n",
      "Epoch 38, Loss: 0.0022\n",
      "Epoch 39, Loss: 0.0021\n",
      "Epoch 40, Loss: 0.0035\n",
      "Epoch 41, Loss: 0.0021\n",
      "Epoch 42, Loss: 0.0021\n",
      "Epoch 43, Loss: 0.0021\n",
      "Epoch 44, Loss: 0.0021\n",
      "Epoch 45, Loss: 0.0021\n",
      "Epoch 46, Loss: 0.0021\n",
      "Epoch 47, Loss: 0.0071\n",
      "Epoch 48, Loss: 0.0036\n",
      "Epoch 49, Loss: 0.0022\n",
      "Epoch 50, Loss: 0.0021\n",
      "Nb. clusters: 2\n"
     ]
    }
   ],
   "source": [
    "# 数据处理部分保持不变\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "# 新增深度学习依赖（放在传统方法导入之前）\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 传统方法模块导入（关键修正点）\n",
    "from dipy.segment.metric import Metric  # 确保在自定义类之前导入\n",
    "from dipy.segment.clustering import ResampleFeature, QuickBundles\n",
    "import geopy.distance\n",
    "THRESHOLD = 1.5\n",
    "# 可视化模块\n",
    "from gmplot import gmplot\n",
    "\n",
    "\n",
    "# 深度学习数据预处理类\n",
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, streams):\n",
    "        # 保持原有归一化逻辑\n",
    "        self.min_lat = min(traj[:,0].min() for traj in streams)\n",
    "        self.max_lat = max(traj[:,0].max() for traj in streams)\n",
    "        self.min_lng = min(traj[:,1].min() for traj in streams)\n",
    "        self.max_lng = max(traj[:,1].max() for traj in streams)\n",
    "        \n",
    "        self.normalized = []\n",
    "        for traj in streams:\n",
    "            norm_traj = np.empty_like(traj)\n",
    "            norm_traj[:,0] = (traj[:,0] - self.min_lat) / (self.max_lat - self.min_lat)\n",
    "            norm_traj[:,1] = (traj[:,1] - self.min_lng) / (self.max_lng - self.min_lng)\n",
    "            self.normalized.append(norm_traj.flatten())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.normalized)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.FloatTensor(self.normalized[idx])\n",
    "\n",
    "# 深度学习模型（保持原有结构）\n",
    "class TrajectoryAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        recon = self.decoder(z)\n",
    "        return z, recon\n",
    "\n",
    "# 修正后的距离度量类\n",
    "class DeepTrajectoryMetric(Metric):  # 确保继承自dipy的Metric\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__(feature=ResampleFeature(nb_points=256))  # 显式调用父类初始化\n",
    "        self.encoder = encoder\n",
    "        self.encoder.eval()\n",
    "        \n",
    "    def are_compatible(self, shape1, shape2):\n",
    "        return len(shape1) == len(shape2)\n",
    "    \n",
    "    def dist(self, v1, v2):\n",
    "        with torch.no_grad():\n",
    "            tensor_v1 = torch.FloatTensor(v1.flatten()).unsqueeze(0)\n",
    "            tensor_v2 = torch.FloatTensor(v2.flatten()).unsqueeze(0)\n",
    "            \n",
    "            z1 = self.encoder(tensor_v1)\n",
    "            z2 = self.encoder(tensor_v2)\n",
    "            \n",
    "            return 1 - torch.cosine_similarity(z1, z2).item()\n",
    "\n",
    "# 主流程整合\n",
    "if __name__ == \"__main__\":\n",
    "    # ===== 数据加载 =====\n",
    "    # ... [原有数据加载代码] ...\n",
    "    \n",
    "    # ===== 数据预处理 =====\n",
    "    # 统一轨迹长度（关键衔接点）\n",
    "    resampler = ResampleFeature(nb_points=256)\n",
    "    streams = [resampler.extract(traj) for traj in streams]\n",
    "    \n",
    "    # ===== 深度学习训练 =====\n",
    "    dataset = TrajectoryDataset(streams)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    model = TrajectoryAE()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    for epoch in range(50):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            _, recon = model(batch)\n",
    "            loss = criterion(recon, batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "    \n",
    "    # ===== 深度聚类 =====\n",
    "    metric = DeepTrajectoryMetric(model.encoder)\n",
    "    qb = QuickBundles(threshold=THRESHOLD, metric=metric)\n",
    "    clusters = qb.cluster(streams)\n",
    "    print(\"Nb. clusters:\", len(clusters))\n",
    "    \n",
    "    # ===== 可视化 =====\n",
    "    def randomcolor():\n",
    "        colorArr = ['1','2','3','4','5','6','7','8','9','A','B','C','D','E','F']\n",
    "        return \"#\" + ''.join(random.choice(colorArr) for _ in range(6))\n",
    "    \n",
    "    gmap = gmplot.GoogleMapPlotter(streams[0][0,0], streams[0][0,1], 12)\n",
    "    \n",
    "    for cluster_id in range(min(7, len(clusters))):  # 防止集群数不足7\n",
    "        color = randomcolor()\n",
    "        for i in clusters[cluster_id].indices:\n",
    "            # 使用原始经纬度数据绘制\n",
    "            lat_points = streams[i][:,0]\n",
    "            lng_points = streams[i][:,1]\n",
    "            gmap.plot(lat_points, lng_points, color, edge_width=1)\n",
    "    \n",
    "    gmap.draw(\"deep_clusters_map2.html\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4856\n",
      "Epoch 2, Loss: 0.4058\n",
      "Epoch 3, Loss: 0.1899\n",
      "Epoch 4, Loss: 0.0998\n",
      "Epoch 5, Loss: 0.0209\n",
      "Epoch 6, Loss: 0.0324\n",
      "Epoch 7, Loss: 0.0176\n",
      "Epoch 8, Loss: 0.0233\n",
      "Epoch 9, Loss: 0.0107\n",
      "Epoch 10, Loss: 0.0099\n",
      "Epoch 11, Loss: 0.0040\n",
      "Epoch 12, Loss: 0.0045\n",
      "Epoch 13, Loss: 0.0029\n",
      "Epoch 14, Loss: 0.0028\n",
      "Epoch 15, Loss: 0.0014\n",
      "Epoch 16, Loss: 0.0015\n",
      "Epoch 17, Loss: 0.0009\n",
      "Epoch 18, Loss: 0.0008\n",
      "Epoch 19, Loss: 0.0006\n",
      "Epoch 20, Loss: 0.0005\n",
      "Epoch 21, Loss: 0.0005\n",
      "Epoch 22, Loss: 0.0003\n",
      "Epoch 23, Loss: 0.0004\n",
      "Epoch 24, Loss: 0.0007\n",
      "Epoch 25, Loss: 0.0002\n",
      "Epoch 26, Loss: 0.0002\n",
      "Epoch 27, Loss: 0.0002\n",
      "Epoch 28, Loss: 0.0001\n",
      "Epoch 29, Loss: 0.0001\n",
      "Epoch 30, Loss: 0.0001\n",
      "Epoch 31, Loss: 0.0001\n",
      "Epoch 32, Loss: 0.0001\n",
      "Epoch 33, Loss: 0.0001\n",
      "Epoch 34, Loss: 0.0001\n",
      "Epoch 35, Loss: 0.0001\n",
      "Epoch 36, Loss: 0.0001\n",
      "Epoch 37, Loss: 0.0002\n",
      "Epoch 38, Loss: 0.0001\n",
      "Epoch 39, Loss: 0.0001\n",
      "Epoch 40, Loss: 0.0001\n",
      "Epoch 41, Loss: 0.0001\n",
      "Epoch 42, Loss: 0.0001\n",
      "Epoch 43, Loss: 0.0000\n",
      "Epoch 44, Loss: 0.0000\n",
      "Epoch 45, Loss: 0.0001\n",
      "Epoch 46, Loss: 0.0000\n",
      "Epoch 47, Loss: 0.0001\n",
      "Epoch 48, Loss: 0.0000\n",
      "Epoch 49, Loss: 0.0000\n",
      "Epoch 50, Loss: 0.0000\n",
      "Nb. clusters: 1\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "   # 数据处理部分保持不变\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "# 新增深度学习依赖（放在传统方法导入之前）\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 传统方法模块导入（关键修正点）\n",
    "from dipy.segment.metric import Metric  # 确保在自定义类之前导入\n",
    "from dipy.segment.clustering import ResampleFeature, QuickBundles\n",
    "import geopy.distance\n",
    "THRESHOLD = 1.5\n",
    "# 可视化模块\n",
    "from gmplot import gmplot\n",
    "\n",
    "\n",
    "# 深度学习数据预处理类\n",
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, streams):\n",
    "        # 保持原有归一化逻辑\n",
    "        self.min_lat = min(traj[:,0].min() for traj in streams)\n",
    "        self.max_lat = max(traj[:,0].max() for traj in streams)\n",
    "        self.min_lng = min(traj[:,1].min() for traj in streams)\n",
    "        self.max_lng = max(traj[:,1].max() for traj in streams)\n",
    "        \n",
    "        self.normalized = []\n",
    "        for traj in streams:\n",
    "            norm_traj = np.empty_like(traj)\n",
    "            norm_traj[:,0] = (traj[:,0] - self.min_lat) / (self.max_lat - self.min_lat)\n",
    "            norm_traj[:,1] = (traj[:,1] - self.min_lng) / (self.max_lng - self.min_lng)\n",
    "            self.normalized.append(norm_traj.flatten())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.normalized)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.FloatTensor(self.normalized[idx])\n",
    "\n",
    "# 深度学习模型（保持原有结构）\n",
    "class TrajectoryAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        recon = self.decoder(z)\n",
    "        return z, recon\n",
    "\n",
    "# 新增注意力机制模块\n",
    "class SpatioTemporalAttention(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = input_dim // num_heads\n",
    "        \n",
    "        self.query = nn.Linear(input_dim, input_dim)\n",
    "        self.key = nn.Linear(input_dim, input_dim)\n",
    "        self.value = nn.Linear(input_dim, input_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape  # [batch, 256, 2]\n",
    "        \n",
    "        # 多头注意力计算\n",
    "        Q = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # 注意力得分\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n",
    "        attn_weights = self.softmax(scores)\n",
    "        \n",
    "        # 上下文向量\n",
    "        context = torch.matmul(attn_weights, V).transpose(1, 2).contiguous()\n",
    "        context = context.view(batch_size, seq_len, -1)\n",
    "        return context\n",
    "\n",
    "# 改进的深度学习模型\n",
    "class AttnTrajectoryAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 编码器\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (256, 2)),  # 恢复时空结构 [batch, 256, 2]\n",
    "            SpatioTemporalAttention(input_dim=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256*2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64)\n",
    "        )\n",
    "        \n",
    "        # 解码器\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256*2),\n",
    "            nn.Unflatten(1, (256, 2)),\n",
    "            SpatioTemporalAttention(input_dim=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 编码阶段\n",
    "        x_reshaped = self.encoder[:3](x)  # [batch, 256, 2]\n",
    "        attn_output = self.encoder[3](x_reshaped)\n",
    "        z = self.encoder[4:](attn_output)\n",
    "        \n",
    "        # 解码阶段\n",
    "        recon = self.decoder(z)\n",
    "        return z, recon\n",
    "\n",
    "# 修改模型初始化部分\n",
    "if __name__ == \"__main__\":\n",
    "    # ... [数据加载和预处理部分不变] ...\n",
    "    \n",
    "    # 使用新模型\n",
    "    model = AttnTrajectoryAE()\n",
    "    \n",
    "    # 修改优化器参数\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    \n",
    "# 修正后的距离度量类\n",
    "class DeepTrajectoryMetric(Metric):  # 确保继承自dipy的Metric\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__(feature=ResampleFeature(nb_points=256))  # 显式调用父类初始化\n",
    "        self.encoder = encoder\n",
    "        self.encoder.eval()\n",
    "        \n",
    "    def are_compatible(self, shape1, shape2):\n",
    "        return len(shape1) == len(shape2)\n",
    "    \n",
    "    def dist(self, v1, v2):\n",
    "        with torch.no_grad():\n",
    "            tensor_v1 = torch.FloatTensor(v1.flatten()).unsqueeze(0)\n",
    "            tensor_v2 = torch.FloatTensor(v2.flatten()).unsqueeze(0)\n",
    "            \n",
    "            z1 = self.encoder(tensor_v1)\n",
    "            z2 = self.encoder(tensor_v2)\n",
    "            \n",
    "            return 1 - torch.cosine_similarity(z1, z2).item()\n",
    "\n",
    "# 主流程整合\n",
    "if __name__ == \"__main__\":\n",
    "    # ===== 数据加载 =====\n",
    "    # ... [原有数据加载代码] ...\n",
    "    \n",
    "    # ===== 数据预处理 =====\n",
    "    # 统一轨迹长度（关键衔接点）\n",
    "    resampler = ResampleFeature(nb_points=256)\n",
    "    streams = [resampler.extract(traj) for traj in streams]\n",
    "    \n",
    "    # ===== 深度学习训练 =====\n",
    "    dataset = TrajectoryDataset(streams)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    model = TrajectoryAE()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    for epoch in range(50):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            _, recon = model(batch)\n",
    "            loss = criterion(recon, batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "    \n",
    "    # ===== 深度聚类 =====\n",
    "    metric = DeepTrajectoryMetric(model.encoder)\n",
    "    qb = QuickBundles(threshold=THRESHOLD, metric=metric)\n",
    "    clusters = qb.cluster(streams)\n",
    "    print(\"Nb. clusters:\", len(clusters))\n",
    "    \n",
    "    # ===== 可视化 =====\n",
    "    def randomcolor():\n",
    "        colorArr = ['1','2','3','4','5','6','7','8','9','A','B','C','D','E','F']\n",
    "        return \"#\" + ''.join(random.choice(colorArr) for _ in range(6))\n",
    "    \n",
    "    gmap = gmplot.GoogleMapPlotter(streams[0][0,0], streams[0][0,1], 12)\n",
    "    \n",
    "    for cluster_id in range(min(7, len(clusters))):  # 防止集群数不足7\n",
    "        color = randomcolor()\n",
    "        for i in clusters[cluster_id].indices:\n",
    "            # 使用原始经纬度数据绘制\n",
    "            lat_points = streams[i][:,0]\n",
    "            lng_points = streams[i][:,1]\n",
    "            gmap.plot(lat_points, lng_points, color, edge_width=1)\n",
    "    \n",
    "    gmap.draw(\"deep_clusters_map4.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
